{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNyJWi0MJGk3Hn+Q/C22+7z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MrSuperfluous/SummerAnal/blob/main/Summeranal_hacky_final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-rKH7e8jU4f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.metrics import classification_report, precision_recall_curve\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "from scipy.signal import savgol_filter, find_peaks\n",
        "from scipy.fft import fft\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "PCA_COMPONENTS = 3\n",
        "THRESHOLDS = {}  # Will be populated later\n",
        "\n",
        "NDVI_COLS = [\n",
        "    '20150720_N','20150602_N','20150517_N','20150501_N','20150415_N','20150330_N',\n",
        "    '20150314_N','20150226_N','20150210_N','20150125_N','20150109_N','20141117_N',\n",
        "    '20141101_N','20141016_N','20140930_N','20140813_N','20140626_N','20140610_N',\n",
        "    '20140525_N','20140509_N','20140423_N','20140407_N','20140322_N','20140218_N',\n",
        "    '20140202_N','20140117_N','20140101_N'\n",
        "]\n"
      ],
      "metadata": {
        "id": "BxGlpWmn5Im_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_time_series(ts):\n",
        "    ts = pd.Series(ts).interpolate(method='linear', limit_direction='both')\n",
        "    if ts.isna().any():\n",
        "        ts.fillna(ts.median(), inplace=True)\n",
        "    if len(ts) > 7:\n",
        "        try:\n",
        "            w = min(7, len(ts) // 2)\n",
        "            if w % 2 == 0: w -= 1\n",
        "            ts = savgol_filter(ts, window_length=w, polyorder=2)\n",
        "        except:\n",
        "            pass\n",
        "    return ts\n",
        "\n",
        "def extract_features(ts):\n",
        "    features = [\n",
        "        np.mean(ts), np.median(ts), np.std(ts), np.ptp(ts),\n",
        "        np.percentile(ts, 25), np.percentile(ts, 75),\n",
        "        np.percentile(ts, 75) - np.percentile(ts, 25)\n",
        "    ]\n",
        "    try:\n",
        "        peaks, _ = find_peaks(ts, distance=3)\n",
        "        features += [\n",
        "            len(peaks),\n",
        "            np.mean(np.diff(peaks)) if len(peaks) > 1 else 0,\n",
        "            np.mean(ts[peaks]) if len(peaks) > 0 else 0\n",
        "        ]\n",
        "    except:\n",
        "        features += [0, 0, 0]\n",
        "    try:\n",
        "        fft_vals = np.abs(fft(ts))\n",
        "        features += [\n",
        "            np.argmax(fft_vals[1:len(fft_vals)//2]) + 1,\n",
        "            np.max(fft_vals),\n",
        "            np.sum(fft_vals[:5])\n",
        "        ]\n",
        "    except:\n",
        "        features += [0, 0, 0]\n",
        "    return features\n"
      ],
      "metadata": {
        "id": "a9rWBl1z5K8F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pipeline(train_df):\n",
        "    X_features, X_series = [], []\n",
        "    for _, row in train_df.iterrows():\n",
        "        ts = process_time_series(row[NDVI_COLS].values.astype(float))\n",
        "        X_series.append(ts)\n",
        "        X_features.append(extract_features(ts))\n",
        "\n",
        "    pca = PCA(n_components=PCA_COMPONENTS, random_state=RANDOM_STATE)\n",
        "    X_pca = pca.fit_transform(X_series)\n",
        "\n",
        "    X = np.hstack([X_features, X_pca])\n",
        "    y = train_df['class']\n",
        "    le = LabelEncoder()\n",
        "    y_encoded = le.fit_transform(y)\n",
        "\n",
        "    selector = SelectKBest(f_classif, k=15)\n",
        "    X_selected = selector.fit_transform(X, y_encoded)\n",
        "\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_selected, y_encoded, test_size=0.2, stratify=y_encoded, random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train_scaled = scaler.fit_transform(X_train)\n",
        "    X_val_scaled = scaler.transform(X_val)\n",
        "\n",
        "    base_model = LogisticRegression(max_iter=2000, class_weight='balanced', solver='saga')\n",
        "    calibrated = CalibratedClassifierCV(base_model, method='isotonic', cv=3)\n",
        "    calibrated.fit(X_train_scaled, y_train)\n",
        "\n",
        "    y_probs = calibrated.predict_proba(X_val_scaled)\n",
        "    y_preds = np.argmax(y_probs, axis=1)\n",
        "\n",
        "    print(\"\\nðŸ“Š Validation Report:\\n\")\n",
        "    print(classification_report(y_val, y_preds, target_names=le.classes_))\n",
        "\n",
        "    # Compute class-wise thresholds\n",
        "    thresholds = {}\n",
        "    for i, class_name in enumerate(le.classes_):\n",
        "        prec, rec, thresh = precision_recall_curve((y_val == i).astype(int), y_probs[:, i])\n",
        "        f1 = 2 * (prec * rec) / (prec + rec + 1e-6)\n",
        "        best_thresh = thresh[np.argmax(f1)]\n",
        "        thresholds[class_name] = best_thresh\n",
        "\n",
        "    return calibrated, scaler, selector, pca, le, thresholds\n"
      ],
      "metadata": {
        "id": "4NKfhKJP5NgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_inference(test_df, model, scaler, selector, pca, le, thresholds):\n",
        "    X_features, X_series = [], []\n",
        "    for _, row in test_df.iterrows():\n",
        "        ts = process_time_series(row[NDVI_COLS].values.astype(float))\n",
        "        X_series.append(ts)\n",
        "        X_features.append(extract_features(ts))\n",
        "\n",
        "    X_pca = pca.transform(X_series)\n",
        "    X = np.hstack([X_features, X_pca])\n",
        "    X_selected = selector.transform(X)\n",
        "    X_scaled = scaler.transform(X_selected)\n",
        "    probs = model.predict_proba(X_scaled)\n",
        "\n",
        "    preds = []\n",
        "    for row in probs:\n",
        "        best_class = None\n",
        "        for i, prob in enumerate(row):\n",
        "            if prob >= thresholds[le.classes_[i]]:\n",
        "                best_class = i\n",
        "                break\n",
        "        if best_class is None:\n",
        "            best_class = np.argmax(row)\n",
        "        preds.append(best_class)\n",
        "\n",
        "    return le.inverse_transform(preds)\n"
      ],
      "metadata": {
        "id": "4SXB1WwB5QO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load data\n",
        "train_df = pd.read_csv(\"train.csv\")\n",
        "test_df = pd.read_csv(\"test.csv\")\n",
        "\n",
        "# Train\n",
        "model, scaler, selector, pca, le, thresholds = train_pipeline(train_df)\n",
        "\n",
        "# Inference\n",
        "predictions = test_inference(test_df, model, scaler, selector, pca, le, thresholds)\n",
        "\n",
        "# Submission\n",
        "submission = pd.DataFrame({\n",
        "    \"ID\": test_df[\"ID\"],\n",
        "    \"class\": predictions\n",
        "})\n",
        "submission.to_csv(\"submission12.csv\", index=False)\n",
        "print(\"âœ… Submission saved successfully as submission.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO1qelnT5ThN",
        "outputId": "050237b2-97df-4e6d-f549-2ce7d0e1ac98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
      ]
    }
  ]
}
